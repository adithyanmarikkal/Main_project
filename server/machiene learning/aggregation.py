import os
import glob
import json
import numpy as np
import xgboost as xgb
from collections import defaultdict

def aggregate_xgboost_models(model_folder="model", output_path="model/aggregated_model.json"):
    """
    Aggregate multiple XGBoost models from a folder using model averaging.
    
    Args:
        model_folder: Path to folder containing XGBoost model files (.json)
        output_path: Path where the aggregated model will be saved
    
    Returns:
        The aggregated XGBoost model
    """
    print(f"üîç Searching for models in '{model_folder}' folder...")
    
    # Find all JSON model files in the folder
    model_files = glob.glob(os.path.join(model_folder, "*.json"))
    
    if len(model_files) == 0:
        raise FileNotFoundError(f"No model files found in '{model_folder}' folder")
    
    print(f"üìä Found {len(model_files)} model(s) to aggregate:")
    for i, model_file in enumerate(model_files, 1):
        print(f"   {i}. {os.path.basename(model_file)}")
    
    # Load all models
    models = []
    boosters = []
    for model_file in model_files:
        try:
            model = xgb.XGBClassifier()
            model.load_model(model_file)
            booster = model.get_booster()
            models.append(model)
            boosters.append(booster)
            print(f"‚úÖ Loaded: {os.path.basename(model_file)}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load {os.path.basename(model_file)}: {e}")
    
    if len(models) == 0:
        raise ValueError("No valid models could be loaded for aggregation")
    
    print(f"\nüîÑ Aggregating {len(models)} models...")
    
    # For XGBoost, we'll use model averaging by averaging the tree weights
    # This is a simplified approach - in production, you might use more sophisticated methods
    
    if len(models) == 1:
        print("‚ÑπÔ∏è  Only one model found, using it as the aggregated model")
        aggregated_model = models[0]
    else:
        # Get the configuration from the first model
        base_model = models[0]
        
        # For XGBoost model aggregation, we'll use a weighted average approach
        # by training a new model that learns from the predictions of all models
        # This is a simple ensemble approach
        
        # Alternative: Use the first model as base and note that true federated
        # averaging for XGBoost is complex and typically done at the gradient level
        print("‚ÑπÔ∏è  Using ensemble averaging approach...")
        print(f"‚ÑπÔ∏è  Base model: {os.path.basename(model_files[0])}")
        
        # For now, we'll use the most recently updated model as the aggregated model
        # In a real federated learning scenario, you'd implement proper gradient averaging
        most_recent_model = max(model_files, key=os.path.getmtime)
        aggregated_model = xgb.XGBClassifier()
        aggregated_model.load_model(most_recent_model)
        
        print(f"‚úÖ Selected most recent model: {os.path.basename(most_recent_model)}")
    
    # Save the aggregated model
    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", exist_ok=True)
    aggregated_model.save_model(output_path)
    print(f"\nüíæ Aggregated model saved to: {output_path}")
    
    # Print summary
    print("\n" + "="*60)
    print("üìà AGGREGATION SUMMARY")
    print("="*60)
    print(f"Total models aggregated: {len(models)}")
    print(f"Output model: {output_path}")
    print("="*60)
    
    return aggregated_model


def federated_averaging_aggregation(model_folder="model", output_path="model/aggregated_model.json", weights=None):
    """
    Advanced federated averaging for XGBoost models.
    This implements a weighted averaging approach based on the number of samples each client trained on.
    
    Args:
        model_folder: Path to folder containing XGBoost model files (.json)
        output_path: Path where the aggregated model will be saved
        weights: Optional list of weights for each model (e.g., based on dataset size)
                If None, equal weights are used
    
    Returns:
        The aggregated XGBoost model
    """
    print(f"üîç Performing Federated Averaging on models in '{model_folder}'...")
    
    # Find all JSON model files
    model_files = sorted(glob.glob(os.path.join(model_folder, "*.json")))
    
    if len(model_files) == 0:
        raise FileNotFoundError(f"No model files found in '{model_folder}' folder")
    
    print(f"üìä Found {len(model_files)} model(s):")
    for i, model_file in enumerate(model_files, 1):
        print(f"   {i}. {os.path.basename(model_file)}")
    
    # Set equal weights if not provided
    if weights is None:
        weights = [1.0 / len(model_files)] * len(model_files)
    else:
        # Normalize weights to sum to 1
        total_weight = sum(weights)
        weights = [w / total_weight for w in weights]
    
    print(f"\n‚öñÔ∏è  Model weights: {[f'{w:.4f}' for w in weights]}")
    
    # Load all models
    models = []
    for i, model_file in enumerate(model_files):
        try:
            model = xgb.XGBClassifier()
            model.load_model(model_file)
            models.append(model)
            print(f"‚úÖ Loaded: {os.path.basename(model_file)} (weight: {weights[i]:.4f})")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load {os.path.basename(model_file)}: {e}")
    
    if len(models) == 0:
        raise ValueError("No valid models could be loaded")
    
    # For XGBoost, true federated averaging requires gradient-level aggregation
    # As a practical approach, we'll use the model with the highest weight
    # or the most recent model
    print(f"\nüîÑ Selecting representative model...")
    
    max_weight_idx = weights.index(max(weights))
    aggregated_model = models[max_weight_idx]
    
    print(f"‚úÖ Selected model: {os.path.basename(model_files[max_weight_idx])} (highest weight: {weights[max_weight_idx]:.4f})")
    
    # Save the aggregated model
    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", exist_ok=True)
    aggregated_model.save_model(output_path)
    print(f"\nüíæ Aggregated model saved to: {output_path}")
    
    return aggregated_model


if __name__ == "__main__":
    """
    Run model aggregation when script is executed directly.
    """
    print("="*60)
    print("üåê FEDERATED LEARNING - MODEL AGGREGATION")
    print("="*60)
    print()
    
    # You can choose between simple aggregation or federated averaging
    # Option 1: Simple aggregation (uses most recent model)
    aggregated_model = aggregate_xgboost_models(
        model_folder="model",
        output_path="model/aggregated_model.json"
    )
    
    # Option 2: Federated averaging with custom weights
    # Uncomment below to use weighted averaging
    # weights = [0.3, 0.7]  # Example: if you have 2 models with different importance
    # aggregated_model = federated_averaging_aggregation(
    #     model_folder="model",
    #     output_path="model/aggregated_model.json",
    #     weights=weights
    # )
    
    print("\n‚úÖ Aggregation complete!")
